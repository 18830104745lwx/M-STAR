#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æˆéƒ½æ»´æ»´æ•°æ®è½¬æ¢ä¸ºLibCityæ ¼å¼ - å®Œæ•´æµç¨‹
æ–¹æ¡ˆB: 20Ã—20ç½‘æ ¼ï¼Œæ ¸å¿ƒå¸‚åŒºï¼Œå¯¹æ ‡TaxiBJ

å‚è€ƒï¼šhttps://bigscity-libcity-docs.readthedocs.io/
"""

import pandas as pd
import numpy as np
import json
import os
import time
from datetime import datetime
from tqdm import tqdm
import util

class ChengduDiDiLibCityConverter:
    def __init__(self, 
                 data_dir='data/2016å¹´11æœˆæˆéƒ½ç½‘çº¦è½¦æ»´æ»´è®¢å•æ•°æ®',
                 output_dir='output/ChengduDiDi20x20',
                 grid_rows=20,
                 grid_cols=20,
                 time_interval=30,
                 use_core_area=True):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            data_dir: åŸå§‹æ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            grid_rows: ç½‘æ ¼è¡Œæ•°
            grid_cols: ç½‘æ ¼åˆ—æ•°
            time_interval: æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
            use_core_area: æ˜¯å¦åªä½¿ç”¨æ ¸å¿ƒ90%å¸‚åŒºæ•°æ®
        """
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.grid_rows = grid_rows
        self.grid_cols = grid_cols
        self.time_interval = time_interval
        self.use_core_area = use_core_area
        self.dataset_name = f'ChengduDiDi{grid_rows}x{grid_cols}'
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        util.ensure_dir(output_dir)
        
        print("=" * 80)
        print("æˆéƒ½æ»´æ»´æ•°æ® -> LibCityæ ¼å¼è½¬æ¢å™¨")
        print("=" * 80)
        print(f"æ•°æ®æº: {data_dir}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ç½‘æ ¼é…ç½®: {grid_rows}Ã—{grid_cols} = {grid_rows*grid_cols}ä¸ªç½‘æ ¼")
        print(f"æ—¶é—´é—´éš”: {time_interval}åˆ†é’Ÿ")
        print(f"ä½¿ç”¨æ ¸å¿ƒåŒºåŸŸ: {use_core_area}")
        print("=" * 80)
        print()
    
    def step1_load_and_clean_data(self, max_files=None):
        """
        æ­¥éª¤1: åŠ è½½å¹¶æ¸…æ´—åŸå§‹æ•°æ®
        """
        print("ã€æ­¥éª¤1/4ã€‘åŠ è½½å¹¶æ¸…æ´—æ•°æ®...")
        print("-" * 80)
        
        # è·å–æ‰€æœ‰CSVæ–‡ä»¶
        files = sorted([f for f in os.listdir(self.data_dir) if f.endswith('.csv')])
        if max_files:
            files = files[:max_files]
        
        print(f"æ‰¾åˆ° {len(files)} ä¸ªCSVæ–‡ä»¶")
        
        # é€ä¸ªåŠ è½½
        data_list = []
        for file in tqdm(files, desc="åŠ è½½æ–‡ä»¶"):
            try:
                file_path = os.path.join(self.data_dir, file)
                df = pd.read_csv(file_path)
                
                # éªŒè¯å¿…è¦çš„åˆ—
                required_cols = ['è®¢å•ID', 'å¼€å§‹è®¡è´¹æ—¶é—´', 'ç»“æŸè®¡è´¹æ—¶é—´', 
                               'ä¸Šè½¦ä½ç½®ç»åº¦', 'ä¸Šè½¦ä½ç½®çº¬åº¦', 'ä¸‹è½¦ä½ç½®ç»åº¦', 'ä¸‹è½¦ä½ç½®çº¬åº¦']
                if all(col in df.columns for col in required_cols):
                    data_list.append(df)
            except Exception as e:
                print(f"è­¦å‘Š: åŠ è½½æ–‡ä»¶ {file} å¤±è´¥: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        print("åˆå¹¶æ•°æ®...")
        raw_data = pd.concat(data_list, ignore_index=True)
        print(f"åŸå§‹æ•°æ®: {len(raw_data):,} æ¡è®°å½•")
        
        # æ•°æ®æ¸…æ´—
        print("\næ•°æ®æ¸…æ´—ä¸­...")
        
        # 1. åˆ é™¤é‡å¤è®¢å•
        raw_data = raw_data.drop_duplicates(subset=['è®¢å•ID'])
        
        # 2. åˆ é™¤ç©ºå€¼
        raw_data = raw_data.dropna(subset=['è®¢å•ID', 'å¼€å§‹è®¡è´¹æ—¶é—´', 'ç»“æŸè®¡è´¹æ—¶é—´',
                                           'ä¸Šè½¦ä½ç½®ç»åº¦', 'ä¸Šè½¦ä½ç½®çº¬åº¦', 
                                           'ä¸‹è½¦ä½ç½®ç»åº¦', 'ä¸‹è½¦ä½ç½®çº¬åº¦'])
        
        # 3. æ—¶é—´æ ¼å¼è½¬æ¢
        raw_data['å¼€å§‹è®¡è´¹æ—¶é—´'] = pd.to_datetime(raw_data['å¼€å§‹è®¡è´¹æ—¶é—´'])
        raw_data['ç»“æŸè®¡è´¹æ—¶é—´'] = pd.to_datetime(raw_data['ç»“æŸè®¡è´¹æ—¶é—´'])
        
        # 4. è¿‡æ»¤æ—¶é—´å¼‚å¸¸
        raw_data = raw_data[raw_data['ç»“æŸè®¡è´¹æ—¶é—´'] > raw_data['å¼€å§‹è®¡è´¹æ—¶é—´']]
        
        # 5. è®¡ç®—è¡Œç¨‹æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
        raw_data['è¡Œç¨‹æ—¶é•¿'] = (raw_data['ç»“æŸè®¡è´¹æ—¶é—´'] - raw_data['å¼€å§‹è®¡è´¹æ—¶é—´']).dt.total_seconds() / 60
        
        # 6. è¿‡æ»¤è¡Œç¨‹æ—¶é•¿å¼‚å¸¸ï¼ˆ2-120åˆ†é’Ÿï¼‰
        raw_data = raw_data[(raw_data['è¡Œç¨‹æ—¶é•¿'] >= 2) & (raw_data['è¡Œç¨‹æ—¶é•¿'] <= 120)]
        
        print(f"æ¸…æ´—åæ•°æ®: {len(raw_data):,} æ¡è®°å½•")
        
        # 7. ç­›é€‰æ ¸å¿ƒå¸‚åŒºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        if self.use_core_area:
            print("\nç­›é€‰æ ¸å¿ƒ90%å¸‚åŒºæ•°æ®...")
            lon_coords = pd.concat([raw_data['ä¸Šè½¦ä½ç½®ç»åº¦'], raw_data['ä¸‹è½¦ä½ç½®ç»åº¦']])
            lat_coords = pd.concat([raw_data['ä¸Šè½¦ä½ç½®çº¬åº¦'], raw_data['ä¸‹è½¦ä½ç½®çº¬åº¦']])
            
            # è®¡ç®—90%åˆ†ä½æ•°èŒƒå›´
            lon_min = np.percentile(lon_coords, 5)
            lon_max = np.percentile(lon_coords, 95)
            lat_min = np.percentile(lat_coords, 5)
            lat_max = np.percentile(lat_coords, 95)
            
            # ç­›é€‰åœ¨æ ¸å¿ƒåŒºåŸŸå†…çš„è®¢å•
            raw_data = raw_data[
                (raw_data['ä¸Šè½¦ä½ç½®ç»åº¦'].between(lon_min, lon_max)) &
                (raw_data['ä¸Šè½¦ä½ç½®çº¬åº¦'].between(lat_min, lat_max)) &
                (raw_data['ä¸‹è½¦ä½ç½®ç»åº¦'].between(lon_min, lon_max)) &
                (raw_data['ä¸‹è½¦ä½ç½®çº¬åº¦'].between(lat_min, lat_max))
            ]
            
            print(f"æ ¸å¿ƒåŒºåŸŸæ•°æ®: {len(raw_data):,} æ¡è®°å½•")
            print(f"ç»åº¦èŒƒå›´: [{lon_min:.6f}, {lon_max:.6f}]")
            print(f"çº¬åº¦èŒƒå›´: [{lat_min:.6f}, {lat_max:.6f}]")
        
        self.clean_data = raw_data
        print(f"\nâœ“ æ­¥éª¤1å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®: {len(raw_data):,} æ¡")
        return raw_data
    
    def step2_create_grid_system(self):
        """
        æ­¥éª¤2: åˆ›å»ºç½‘æ ¼ç³»ç»Ÿ
        """
        print("\nã€æ­¥éª¤2/4ã€‘åˆ›å»ºç½‘æ ¼ç³»ç»Ÿ...")
        print("-" * 80)
        
        df = self.clean_data
        
        # è®¡ç®—æ•°æ®çš„ç»çº¬åº¦èŒƒå›´
        lon_min = df[['ä¸Šè½¦ä½ç½®ç»åº¦', 'ä¸‹è½¦ä½ç½®ç»åº¦']].min().min()
        lon_max = df[['ä¸Šè½¦ä½ç½®ç»åº¦', 'ä¸‹è½¦ä½ç½®ç»åº¦']].max().max()
        lat_min = df[['ä¸Šè½¦ä½ç½®çº¬åº¦', 'ä¸‹è½¦ä½ç½®çº¬åº¦']].min().min()
        lat_max = df[['ä¸Šè½¦ä½ç½®çº¬åº¦', 'ä¸‹è½¦ä½ç½®çº¬åº¦']].max().max()
        
        # åˆ›å»ºç½‘æ ¼è¾¹ç•Œ
        lon_bins = np.linspace(lon_min, lon_max, self.grid_cols + 1)
        lat_bins = np.linspace(lat_min, lat_max, self.grid_rows + 1)
        
        print(f"ç½‘æ ¼é…ç½®: {self.grid_rows}è¡Œ Ã— {self.grid_cols}åˆ— = {self.grid_rows * self.grid_cols}ä¸ªç½‘æ ¼")
        
        # ä¼°ç®—æ¯ä¸ªç½‘æ ¼çš„å®é™…å¤§å°
        lon_per_grid = (lon_max - lon_min) / self.grid_cols
        lat_per_grid = (lat_max - lat_min) / self.grid_rows
        grid_width_km = lon_per_grid * 96.5  # æˆéƒ½çº¦åŒ—çº¬30åº¦
        grid_height_km = lat_per_grid * 111.0
        print(f"å•ä¸ªç½‘æ ¼å¤§å°: çº¦ {grid_width_km:.2f} km Ã— {grid_height_km:.2f} km")
        
        # ä¸ºè®¢å•åˆ†é…ç½‘æ ¼ID
        print("\nä¸ºè®¢å•åˆ†é…ç½‘æ ¼ID...")
        df['pickup_grid_col'] = pd.cut(df['ä¸Šè½¦ä½ç½®ç»åº¦'], lon_bins, labels=False, include_lowest=True)
        df['pickup_grid_row'] = pd.cut(df['ä¸Šè½¦ä½ç½®çº¬åº¦'], lat_bins, labels=False, include_lowest=True)
        df['dropoff_grid_col'] = pd.cut(df['ä¸‹è½¦ä½ç½®ç»åº¦'], lon_bins, labels=False, include_lowest=True)
        df['dropoff_grid_row'] = pd.cut(df['ä¸‹è½¦ä½ç½®çº¬åº¦'], lat_bins, labels=False, include_lowest=True)
        
        # è®¡ç®—ç½‘æ ¼IDï¼ˆrow * n_cols + colï¼‰
        df['pickup_grid_id'] = df['pickup_grid_row'] * self.grid_cols + df['pickup_grid_col']
        df['dropoff_grid_id'] = df['dropoff_grid_row'] * self.grid_cols + df['dropoff_grid_col']
        
        # è¿‡æ»¤åˆ†é…å¤±è´¥çš„è®°å½•
        before = len(df)
        df = df.dropna(subset=['pickup_grid_id', 'dropoff_grid_id'])
        df['pickup_grid_id'] = df['pickup_grid_id'].astype(int)
        df['dropoff_grid_id'] = df['dropoff_grid_id'].astype(int)
        after = len(df)
        
        print(f"ç½‘æ ¼åˆ†é…æˆåŠŸ: {after:,}/{before:,} æ¡è®°å½•")
        
        # ä¿å­˜ç½‘æ ¼ä¿¡æ¯
        self.grid_info = {
            'lon_bins': lon_bins,
            'lat_bins': lat_bins,
            'lon_min': lon_min,
            'lon_max': lon_max,
            'lat_min': lat_min,
            'lat_max': lat_max,
            'grid_rows': self.grid_rows,
            'grid_cols': self.grid_cols,
            'n_grids': self.grid_rows * self.grid_cols
        }
        
        self.gridded_data = df
        print(f"\nâœ“ æ­¥éª¤2å®Œæˆ")
        return df
    
    def step3_aggregate_flow(self):
        """
        æ­¥éª¤3: æ—¶ç©ºæµé‡èšåˆ
        """
        print("\nã€æ­¥éª¤3/4ã€‘æ—¶ç©ºæµé‡èšåˆ...")
        print("-" * 80)
        
        df = self.gridded_data
        
        # æ—¶é—´å¤„ç†
        df['date'] = df['å¼€å§‹è®¡è´¹æ—¶é—´'].dt.date
        start_date = df['date'].min()
        end_date = df['date'].max()
        
        print(f"æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        
        # åˆ›å»ºæ—¶é—´çª—å£ID
        slots_per_day = 24 * 60 // self.time_interval
        print(f"æ¯å¤©æ—¶é—´çª—å£æ•°: {slots_per_day}")
        
        # è®¡ç®—å…¨å±€æ—¶é—´ç´¢å¼•
        def get_time_idx(row):
            days = (row['date'] - start_date).days
            minutes = row['å¼€å§‹è®¡è´¹æ—¶é—´'].hour * 60 + row['å¼€å§‹è®¡è´¹æ—¶é—´'].minute
            slot = minutes // self.time_interval
            return days * slots_per_day + slot
        
        print("è®¡ç®—æ—¶é—´ç´¢å¼•...")
        df['time_idx'] = df.apply(get_time_idx, axis=1)
        
        max_time_idx = df['time_idx'].max()
        n_timesteps = max_time_idx + 1
        
        print(f"æ€»æ—¶é—´æ­¥æ•°: {n_timesteps}")
        
        # ç»Ÿè®¡inflowå’Œoutflow
        print("\nç»Ÿè®¡ç½‘æ ¼æµé‡...")
        
        # Inflow: ä»¥è¯¥ç½‘æ ¼ä¸ºç»ˆç‚¹çš„å‡ºè¡Œæ¬¡æ•°ï¼ˆä¸‹è½¦ç‚¹æ•°é‡ï¼‰
        inflow = df.groupby(['time_idx', 'dropoff_grid_id']).size().reset_index(name='inflow')
        
        # Outflow: ä»¥è¯¥ç½‘æ ¼ä¸ºèµ·ç‚¹çš„å‡ºè¡Œæ¬¡æ•°ï¼ˆä¸Šè½¦ç‚¹æ•°é‡ï¼‰
        outflow = df.groupby(['time_idx', 'pickup_grid_id']).size().reset_index(name='outflow')
        
        # åˆ›å»ºå®Œæ•´çš„æ—¶ç©ºç´¢å¼•ï¼ˆç¡®ä¿æ‰€æœ‰æ—¶é—´å’Œç½‘æ ¼çš„ç»„åˆéƒ½å­˜åœ¨ï¼‰
        print("ç”Ÿæˆå®Œæ•´æ—¶ç©ºçŸ©é˜µ...")
        time_indices = range(n_timesteps)
        grid_indices = range(self.grid_info['n_grids'])
        
        # åˆ›å»ºå®Œæ•´ç´¢å¼•
        full_index = pd.MultiIndex.from_product(
            [time_indices, grid_indices],
            names=['time_idx', 'grid_id']
        )
        
        # é‡æ–°ç´¢å¼•inflowï¼ˆå¡«å……0ï¼‰
        inflow_full = inflow.set_index(['time_idx', 'dropoff_grid_id'])['inflow'].reindex(
            full_index, fill_value=0
        ).reset_index()
        inflow_full.columns = ['time_idx', 'grid_id', 'inflow']
        
        # é‡æ–°ç´¢å¼•outflowï¼ˆå¡«å……0ï¼‰
        outflow_full = outflow.set_index(['time_idx', 'pickup_grid_id'])['outflow'].reindex(
            full_index, fill_value=0
        ).reset_index()
        outflow_full.columns = ['time_idx', 'grid_id', 'outflow']
        
        # åˆå¹¶
        flow_data = pd.merge(inflow_full, outflow_full, on=['time_idx', 'grid_id'])
        
        print(f"æµé‡çŸ©é˜µå½¢çŠ¶: {flow_data.shape}")
        print(f"å¹³å‡inflow: {flow_data['inflow'].mean():.2f}")
        print(f"å¹³å‡outflow: {flow_data['outflow'].mean():.2f}")
        
        # è®¡ç®—é›¶å€¼ç‡
        total_cells = len(flow_data)
        zero_cells = len(flow_data[(flow_data['inflow'] == 0) & (flow_data['outflow'] == 0)])
        zero_rate = zero_cells / total_cells * 100
        print(f"é›¶å€¼ç‡: {zero_rate:.2f}%")
        
        self.flow_data = flow_data
        self.time_info = {
            'start_date': start_date,
            'end_date': end_date,
            'n_timesteps': n_timesteps,
            'slots_per_day': slots_per_day
        }
        
        print(f"\nâœ“ æ­¥éª¤3å®Œæˆ")
        return flow_data
    
    def step4_convert_to_libcity(self):
        """
        æ­¥éª¤4: è½¬æ¢ä¸ºLibCityæ ¼å¼
        """
        print("\nã€æ­¥éª¤4/4ã€‘è½¬æ¢ä¸ºLibCityæ ¼å¼...")
        print("-" * 80)
        
        # 4.1 ç”Ÿæˆ .geo æ–‡ä»¶
        print("\nç”Ÿæˆ .geo æ–‡ä»¶...")
        self._generate_geo_file()
        
        # 4.2 ç”Ÿæˆ .grid æ–‡ä»¶
        print("ç”Ÿæˆ .grid æ–‡ä»¶...")
        self._generate_grid_file()
        
        # 4.3 ç”Ÿæˆ config.json æ–‡ä»¶
        print("ç”Ÿæˆ config.json æ–‡ä»¶...")
        self._generate_config_file()
        
        print(f"\nâœ“ æ­¥éª¤4å®Œæˆ")
        print("\n" + "=" * 80)
        print("è½¬æ¢å®Œæˆï¼")
        print("=" * 80)
        print(f"è¾“å‡ºæ–‡ä»¶ä½äº: {self.output_dir}")
        print(f"  â€¢ {self.dataset_name}.geo")
        print(f"  â€¢ {self.dataset_name}.grid")
        print(f"  â€¢ config.json")
        print()
    
    def _generate_geo_file(self):
        """ç”Ÿæˆ.geoæ–‡ä»¶ï¼ˆç½‘æ ¼åœ°ç†ä¿¡æ¯ï¼‰"""
        geo_data = []
        
        lon_bins = self.grid_info['lon_bins']
        lat_bins = self.grid_info['lat_bins']
        
        for row_id in range(self.grid_rows):
            for col_id in range(self.grid_cols):
                geo_id = row_id * self.grid_cols + col_id
                
                # æ„å»ºå¤šè¾¹å½¢åæ ‡ï¼ˆç»çº¬åº¦æ ¼å¼ï¼‰
                lon_left = lon_bins[col_id]
                lon_right = lon_bins[col_id + 1]
                lat_bottom = lat_bins[row_id]
                lat_top = lat_bins[row_id + 1]
                
                # LibCityçš„Polygonæ ¼å¼: [[lon, lat], ...]
                coordinates = [[
                    [lon_left, lat_bottom],
                    [lon_right, lat_bottom],
                    [lon_right, lat_top],
                    [lon_left, lat_top],
                    [lon_left, lat_bottom]  # é—­åˆ
                ]]
                
                geo_data.append({
                    'geo_id': geo_id,
                    'type': 'Polygon',
                    'coordinates': str(coordinates),
                    'row_id': row_id,
                    'column_id': col_id
                })
        
        geo_df = pd.DataFrame(geo_data)
        geo_file = os.path.join(self.output_dir, f'{self.dataset_name}.geo')
        geo_df.to_csv(geo_file, index=False)
        print(f"  ä¿å­˜: {geo_file} ({len(geo_df)} ä¸ªç½‘æ ¼)")
    
    def _generate_grid_file(self):
        """ç”Ÿæˆ.gridæ–‡ä»¶ï¼ˆæ—¶ç©ºæµé‡æ•°æ®ï¼‰"""
        flow_data = self.flow_data.copy()
        
        # è½¬æ¢æ—¶é—´ç´¢å¼•ä¸ºISOæ ¼å¼æ—¶é—´
        start_date = self.time_info['start_date']
        slots_per_day = self.time_info['slots_per_day']
        
        def time_idx_to_datetime(time_idx):
            days = time_idx // slots_per_day
            slot_in_day = time_idx % slots_per_day
            hours = (slot_in_day * self.time_interval) // 60
            minutes = (slot_in_day * self.time_interval) % 60
            
            dt = datetime.combine(start_date, datetime.min.time())
            dt = dt.replace(hour=hours, minute=minutes)
            dt = dt + pd.Timedelta(days=days)
            
            return dt.strftime('%Y-%m-%dT%H:%M:%SZ')
        
        print("  è½¬æ¢æ—¶é—´æ ¼å¼...")
        flow_data['time'] = flow_data['time_idx'].apply(time_idx_to_datetime)
        
        # æå–row_idå’Œcolumn_id
        flow_data['row_id'] = flow_data['grid_id'] // self.grid_cols
        flow_data['column_id'] = flow_data['grid_id'] % self.grid_cols
        
        # æ„å»ºLibCity .gridæ ¼å¼
        grid_data = flow_data[['time_idx', 'time', 'row_id', 'column_id', 'inflow', 'outflow']].copy()
        grid_data.insert(0, 'dyna_id', range(len(grid_data)))
        grid_data.insert(1, 'type', 'state')
        grid_data = grid_data[['dyna_id', 'type', 'time', 'row_id', 'column_id', 'inflow', 'outflow']]
        
        # ä¿å­˜
        grid_file = os.path.join(self.output_dir, f'{self.dataset_name}.grid')
        grid_data.to_csv(grid_file, index=False)
        print(f"  ä¿å­˜: {grid_file} ({len(grid_data):,} æ¡è®°å½•)")
    
    def _generate_config_file(self):
        """ç”Ÿæˆconfig.jsonæ–‡ä»¶"""
        config = {
            'geo': {
                'including_types': ['Polygon'],
                'Polygon': {
                    'row_id': 'num',
                    'column_id': 'num'
                }
            },
            'grid': {
                'including_types': ['state'],
                'state': {
                    'row_id': self.grid_rows,
                    'column_id': self.grid_cols,
                    'inflow': 'num',
                    'outflow': 'num'
                }
            },
            'info': {
                'data_col': ['inflow', 'outflow'],
                'data_files': [self.dataset_name],
                'geo_file': self.dataset_name,
                'output_dim': 2,
                'time_intervals': self.time_interval * 60,  # è½¬æ¢ä¸ºç§’
                'init_weight_inf_or_zero': 'inf',
                'set_weight_link_or_dist': 'dist',
                'calculate_weight_adj': False,
                'weight_adj_epsilon': 0.1
            }
        }
        
        config_file = os.path.join(self.output_dir, 'config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        print(f"  ä¿å­˜: {config_file}")
    
    def generate_statistics_report(self):
        """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 80)
        
        flow_data = self.flow_data
        
        print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯")
        print(f"  æ•°æ®é›†åç§°: {self.dataset_name}")
        print(f"  åŒºåŸŸ: æˆéƒ½å¸‚æ ¸å¿ƒåŒº")
        print(f"  æ—¶é—´èŒƒå›´: {self.time_info['start_date']} ~ {self.time_info['end_date']}")
        
        n_days = (self.time_info['end_date'] - self.time_info['start_date']).days + 1
        print(f"  å¤©æ•°: {n_days} å¤©")
        
        print(f"\nğŸ”¢ æ•°æ®è§„æ¨¡")
        print(f"  ç½‘æ ¼é…ç½®: {self.grid_rows}Ã—{self.grid_cols} = {self.grid_info['n_grids']} ä¸ªç½‘æ ¼")
        print(f"  æ—¶é—´æ­¥æ•°: {self.time_info['n_timesteps']}")
        print(f"  æ—¶é—´é—´éš”: {self.time_interval} åˆ†é’Ÿ")
        print(f"  æ¯æ—¥æ—¶é—´çª—å£: {self.time_info['slots_per_day']}")
        print(f"  æ€»è®°å½•æ•°: {len(flow_data):,}")
        
        print(f"\nğŸ“ˆ æµé‡ç»Ÿè®¡")
        print(f"  æ€»trips: {int(flow_data['inflow'].sum()):,}")
        print(f"  å¹³å‡inflow: {flow_data['inflow'].mean():.2f} æ¡/ç½‘æ ¼/æ—¶æ®µ")
        print(f"  å¹³å‡outflow: {flow_data['outflow'].mean():.2f} æ¡/ç½‘æ ¼/æ—¶æ®µ")
        print(f"  æœ€å¤§inflow: {int(flow_data['inflow'].max())}")
        print(f"  æœ€å¤§outflow: {int(flow_data['outflow'].max())}")
        
        # é›¶å€¼ç‡
        zero_cells = len(flow_data[(flow_data['inflow'] == 0) & (flow_data['outflow'] == 0)])
        zero_rate = zero_cells / len(flow_data) * 100
        print(f"  é›¶å€¼ç‡: {zero_rate:.2f}%")
        
        # æ´»è·ƒç½‘æ ¼
        active_grids = flow_data.groupby('grid_id')[['inflow', 'outflow']].sum()
        active_count = len(active_grids[(active_grids['inflow'] > 0) | (active_grids['outflow'] > 0)])
        print(f"  æ´»è·ƒç½‘æ ¼æ•°: {active_count}/{self.grid_info['n_grids']}")
        
        print(f"\nğŸ“ ç½‘æ ¼ä¿¡æ¯")
        lon_span = (self.grid_info['lon_max'] - self.grid_info['lon_min']) * 96.5
        lat_span = (self.grid_info['lat_max'] - self.grid_info['lat_min']) * 111.0
        print(f"  è¦†ç›–èŒƒå›´: {lon_span:.2f} km Ã— {lat_span:.2f} km")
        grid_w = lon_span / self.grid_cols
        grid_h = lat_span / self.grid_rows
        print(f"  å•ç½‘æ ¼å¤§å°: {grid_w:.2f} km Ã— {grid_h:.2f} km")
        
        print()


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    converter = ChengduDiDiLibCityConverter(
        data_dir='data/2016å¹´11æœˆæˆéƒ½ç½‘çº¦è½¦æ»´æ»´è®¢å•æ•°æ®',
        output_dir='output/ChengduDiDi20x20',
        grid_rows=20,
        grid_cols=20,
        time_interval=30,
        use_core_area=True
    )
    
    # æ‰§è¡Œè½¬æ¢æµç¨‹
    try:
        # æ­¥éª¤1: åŠ è½½å’Œæ¸…æ´—æ•°æ®
        converter.step1_load_and_clean_data(max_files=None)  # None=ä½¿ç”¨æ‰€æœ‰æ–‡ä»¶
        
        # æ­¥éª¤2: åˆ›å»ºç½‘æ ¼ç³»ç»Ÿ
        converter.step2_create_grid_system()
        
        # æ­¥éª¤3: æ—¶ç©ºæµé‡èšåˆ
        converter.step3_aggregate_flow()
        
        # æ­¥éª¤4: è½¬æ¢ä¸ºLibCityæ ¼å¼
        converter.step4_convert_to_libcity()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        converter.generate_statistics_report()
        
        print("âœ… è½¬æ¢æˆåŠŸï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. å°†outputç›®å½•ä¸­çš„æ–‡ä»¶å¤åˆ¶åˆ°LibCityçš„æ•°æ®ç›®å½•")
        print("2. åœ¨LibCityé…ç½®æ–‡ä»¶ä¸­æŒ‡å®šæ•°æ®é›†åç§°: ChengduDiDi20x20")
        print("3. è¿è¡Œæ‚¨çš„äº¤é€šé¢„æµ‹æ¨¡å‹")
        
    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

